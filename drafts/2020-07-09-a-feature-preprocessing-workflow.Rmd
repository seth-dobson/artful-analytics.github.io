---
title: "A Feature Preprocessing Workflow"
author: "Seth Dobson"
date: "7/9/2020"
output: 
  md_document: 
    variant: markdown_github
---

How I deal with wide datasets when building a predictive model

## Introduction

In this post, I will describe a preprocessing workflow that I use whenever I have a lot variables (wide data) and need to build a predictive model quickly.

The workflow has three stages:

* Univariate feature selection using the {Information} package
* Feature engineering using the {vtreat} package
* Removal of redundant features using the {caret} package

The overall goal of the approach described here is to provide a reasonable number of highly relevent and non-redundant inputs to tree-based classification algorithms, such as random forests or gradient boosting machines.

To show how it works, let's start by loading the necessary packages, and then get some example data.

```{r load packages, message=FALSE}
# load packages
library(dplyr)
library(Information)
library(rsample)
library(caret)
library(tidyselect)
library(vtreat)
library(stringr)
```

## Example data

I will use a dataset from the {Information} package to illustrate the workflow (actually two datasets, one called `train` and the other called `valid`). The data represent a marketing campaign with a treat-control design. 

If we limit the dataset to the treat group, that gives us >10,000 records and 70 variables. The response variable `purchase` is 1 or 0 depending on whether the customer made a purchase or not. The predictors are mainly credit bureau variables. 

Since the dataset is clean (all numeric), I'm going to dirty it up a bit by making `unique_id` a character variable, and grouping the `d_region` indicators into one character variable with 4 values.

```{r example data}
# get example datasets
df1 <- Information::train
df2 <- Information::valid

# combine and dirty up
df <- df1 %>%
  bind_rows(df2) %>% 
  rename_with(~str_to_lower(.)) %>% 
  filter(treatment == 1) %>% 
  select(-treatment) %>% 
  mutate(
    unique_id = as.character(unique_id),
    d_region = case_when(
      d_region_a == 1 ~ "a",
      d_region_b == 1 ~"b",
      d_region_c == 1 ~ "c",
      TRUE  ~ "d"
    )
  ) %>% 
  select(-c("d_region_a", "d_region_b", "d_region_c"))

rm(list = c("df1", "df2"))
```

## Data partitioning

**Never use the same data for feature preprocessing and model training as this could result in nested model bias.** Instead, do a three-way split. For example, I will use 60% of the example data for model training, 20% for feature preprocessing, and 20% for testing. See [this article](https://win-vector.com/2016/04/26/on-nested-models/) for more information about nested model bias and how to avoid it.

```{r split data}
set.seed(12345)

# split train vs. the rest
split1 <- initial_split(df, 0.6, strata = purchase)
df_train <- training(split1)
df_split2 <- testing(split1)

# split preprocessing vs. test
split2 <- initial_split(df_split2, 0.5, strata = purchase)
df_pre <- training(split2)
df_test <- testing(split2)

rm(list = c("df", "df_split2", "split1", "split2"))
```

Check to make the sure the split worked properly by seeing if the response variable mean is the same between samples.

```{r check response}
tibble(
  pre = mean(df_pre$purchase),
  train = mean(df_train$purchase),
  test = mean(df_test$purchase)
)
```

## Information value

Information value (IV) is a highly flexible approach that lets you measure the strength of association betweeen the response and each predictor. It's a good way to filter out irrelevant variables prior to building a model.

There are several advantages of IV over other filtering methods. 

* IV detect linear **and** non-linear relationships
* IV scores allow you to directly compare continuous and categorical variables 
* IV can handle missing data without imputation and assess the predictive power of NAs

It is good practice to split the preprocessing dataset prior to estimating IV. This allows you to adjust the IV estimates using cross-validation to prevent weak predictors from getting past the filter by chance. See the {Information} package [vignette](https://cran.r-project.org/web/packages/Information/vignettes/Information-vignette.html) for more details.

```{r information value, eval=FALSE}
set.seed(666)

# split preprocessin data
iv_split <- initial_split(df_pre, 0.5, strata = "purchase")
df_iv_train <- training(iv_split)
df_iv_test <- testing(iv_split)

# calculate IV
iv <- create_infotables(
  data = df_iv_train,
  valid = df_iv_test,
  y = "purchase"
)
```

Note that the `unique_id` variable was ignored because it has too many levels. This is a handy feature of the {Information} package when dealing with large datasets. It automatically ignores "junk" variables, like customer IDs and zip codes. Any feature that's non-numeric with more than 1,000 levels gets excluded.

The `create_infotables()` function will create a data frame (accessible via `iv$Summary`) with an IV estimate for each predictor, along with a cross-validation penalty, and the adjusted IV score.

Once you have the IV estimates, you will need to pick a threshold for excluding variables based on adjusted IV. This is subjective. But in general, the rule of thumb is:

| IV  |  Predictive Power |
|---|---|
| <0.02  |  useless |
| 0.02 to 0.1  |  weak |
| 0.1 to 0.3  | medium  |
| 0.3 to 0.5  |  strong |
|  >0.5 |  suspicious |

**You don't want to be too restrictive at this stage**, especially if you are using a modeling approach that has a built-in feature selection process, as is the case with tree-based algorithms. Typically, I would drop all variables with adjusted IV <0.02. However, if most of the variables have relatively low IV scores, I would take the `top_n()` and hope for the best.

```{r include=FALSE}
iv <- readRDS("/cloud/project/drafts/iv.RDS")
```


```{r top predictors}
# get top predictors
top_iv <- iv$Summary  %>% 
  filter(AdjIV > 0.02) 

# save predictor names for filtering
top_nm <- as.character(top_iv$Variable)

top_iv
```

As you can see, filtering by adjusted IV reduces the number of predictors in our example dataset to 41. That implies that 37% of the original 65 predictors where probably "useless."

## Feature engineering

[{vtreat}](https://winvector.github.io/vtreat/) is my go-to R package for common feature engineering tasks. For a formal description of the package read [this paper](https://arxiv.org/abs/1611.09477). The package is also described in the excellent book [*Practical Data Science with R*](https://www.manning.com/books/practical-data-science-with-r-second-edition) by the package authors Nina Zumel and John Mount.

The {vtreat} package has functions that will automatically:

* Replace NAs with the column mean value (numeric) or majority class (non-numeric)
* Create missing-indicator variables
* Dummy code all non-numeric variables with frequency >2% (rare levels get grouped together)
* Truncate numeric distributions to mitigate outliers
* Create derived versions of non-numeric variables using prevalance coding and [impact coding](https://win-vector.com/2012/07/23/modeling-trick-impact-coding-of-categorical-variables-with-many-levels/)

Prevalence coding replaces the levels of a categorical variable with the proportion each level is observed in the dataset. Impact coding uses the marginal effect from a single-variable logistic regression as a replacement for each level in a categorical variable. Both derived variables are numeric.

### Create treatment plan

Use the `designTreatmentsC()` function to create a variable treatment plan for classification models. There are a lot of arguments for this function, so check the documentation. Save the treatment plan (`vtreat_plan`) as an .RDS object so you can apply it to non-training data prior to generating model predictions.

```{r create treatment plan}
# filter preprocessing data by IV
df_vtreat <- df_pre %>% 
  select(all_of(top_nm), purchase) 

# create plan
vtreat_plan <- designTreatmentsC(
  dframe = df_vtreat,
  varlist = top_nm,
  outcomename = "purchase",
  outcometarget = 1,
  collarProb = .025
)
```

### Prepare training data

After you have the treatment plan object, you can apply it to a new dataset using `prepare()`. This creates a new data frame with treated variables based on the `codeRestriction` argument. Read [this](https://cran.r-project.org/web/packages/vtreat/vignettes/vtreatVariableTypes.html) for a description of the different {vtreat} variable types.

```{r treat training data}
# created treated data frame
df_train2 <- prepare(
  treatmentplan = vtreat_plan,
  dframe = df_train,
  codeRestriction = c("clean", "lev", "catB", "catP", "isBAD"),
  doCollar = TRUE
)
```

### Explore derived variables

```{r check d_region}
# check the d_region variable
df_train2 %>% 
  select(contains("d_region")) %>% 
  head()
```

Notice that the original `d_region` character variable has been transformed into 4 dummy indicators, plus 2 derived variables based on prevalence ("catP") and impact coding ("catB"). The `prepare()` function will do this automatically for every non-numeric variable in the dataset.

## Remove redundant variables

Redundant variables are predictors that are highly correlated with one or more other predictors in the dataset.

From a predictive accuracy standpoint, it is not strictly necessary to remove redundant variables prior to model fitting. This is one of the many distinctions between [predictive and explanatory modeling](https://www.stat.berkeley.edu/~aldous/157/Papers/shmueli.pdf).

However, when using tree-based algorithms, it is necessary to remove redundant predictors in order to get accurate variable importance rankings, which helps with interpretability. To find the most redudant features in a dataset, I use the [findCorrelation()](https://topepo.github.io/caret/pre-processing.html#identifying-correlated-predictors) function from the [{Caret}](https://topepo.github.io/caret/) package.

```{r find correlated}
# get names of redundant predictors
corr_vars <- findCorrelation(
  cor(
    df_train2,
    method = "spearman"
  ),
  cutoff = 0.9,
  names = TRUE,
  exact = TRUE
)

corr_vars
```

## And voila...

```{r final dataset}
# filter out redundant predictors
df_train3 <- df_train2 %>% select(-all_of(corr_vars))

str(df_train3)
```

After filtering by IV, prepping with {vtreat}, and removing `r length(corr_vars)` redundant variables, the final dataset has 43 predictors that are all numeric and ready for model training.

## Conclusion

The preprocessing workflow I described here works well for the sorts of modeling projects I work on because it's basically an excercie in [data mining](https://en.wikipedia.org/wiki/Data_mining). But if you have a relatively small set of well-understood predictors, the methods for variable selection and data reduction described in Frank Harrell's book [*Regression Modeling Strategies*](https://link.springer.com/book/10.1007/978-3-319-19425-7) might be more appropriate. Also, to take your preprocessing workflow to the next level, consider using the tidymodels package [{recipes}](https://recipes.tidymodels.org/index.html).

## Questions or comments?

Feel free to reach out to me at any of the social links below.

**For more R content, please visit [R-bloggers](https://www.r-bloggers.com/) and [RWeekly.org](https://rweekly.org/).**