---
title: "A Feature Preprocessing Workflow"
author: "Seth Dobson"
date: "6/30/2020"
output: 
  md_document: 
    variant: markdown_github
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

How I deal with wide datasets when building a predictive model

## Introduction

In this post, I will describe a preprocessing workflow that I use whenever I have a lot variables (wide data), and I need to build a predictive model.

The workflow has three stages:

* Univariate feature selection using the {Information} package
* Feature engineering using the {vtreat} package
* Removal of redundant features using the {caret} package

The overall goal of the approach described here is to provide a reasonable number of highly relevent and non-redundant inputs to tree-based classifcation algorithms, such as random forests or gradient boosting machines.

To show how it works, let's start by loading the necessary packages, and then get some example data.

```{r load packages}
# load packages
library(dplyr)
library(Information)
library(rsample)
library(caret)
library(tidyselect)
library(vtreat)
library(stringr)
```

## Example data

I will use a dataset from the {Information} package to illustrate the workflow (actually two datasets, one called `train` and the other called `valid`). The data represent a marketing campaign with a treat-control design. If we limit the dataset to the treated group, that gives us >10,000 records and 70 variables. The response variable `purchase` is 1 or 0 depending on whether the customer made a purchase or not. The predictors are mainly credit bureau variables. 

Since the dataset is clean (all numeric), I'm going to dirty it up a bit by making `unique_id` a character variable, and grouping the `d_region` indicators into one character variable with 4 values.

```{r example data}
# get example datasets
df1 <- Information::train
df2 <- Information::valid

# combine into one data frame and dirty it up
df <- df1 %>%
  bind_rows(df2) %>% 
  rename_with(~str_to_lower(.)) %>% 
  filter(treatment == 1) %>% 
  select(-treatment) %>% 
  mutate(
    unique_id = as.character(unique_id),
    d_region = case_when(
      d_region_a == 1 ~ "a",
      d_region_b == 1 ~"b",
      d_region_c == 1 ~ "c",
      TRUE  ~ "d"
    )
  ) %>% 
  select(-c("d_region_a", "d_region_b", "d_region_c"))

rm(list = c("df1", "df2"))
```

## Data partitioning

**Never use the same data for feature preprocessing and model training as this could result in nested model bias.** Instead, do a three-way split. For example, I will use 60% of the example data for model training, 20% for feature preprocessing, and 20% for testing. See [this article](https://win-vector.com/2016/04/26/on-nested-models/) for more information about nested model bias and how to avoid it.

```{r split data}
set.seed(12345)

# create split between train and the rest
split1 <- initial_split(df, 0.6, strata = purchase)
df_train <- training(split1)
df_split2 <- testing(split1)

# create split between preprocessing and test
split2 <- initial_split(df_split2, 0.5, strata = purchase)
df_pre <- training(split2)
df_test <- testing(split2)

rm(list = c("df", "df_split2", "split1", "split2"))
```

Check to make the sure the split worked properly by seeing if the response variable mean is the same between samples.

```{r check response}
tibble(
  pre = mean(df_pre$purchase),
  train = mean(df_train$purchase),
  test = mean(df_test$purchase)
)
```

## Information value

Information value (IV) is a highly flexible approach that lets you measure the strength of association betweeen the response and each predictor individually. It's a good way to filter out irrelevant variables prior to building a model.

There are several advantages of IV over other filtering methods. 

* IV detect linear **and** non-linear relationships
* IV scores allow you to directly compare continuous and categorical variables 
* IV can handle missing data without imputation and assess the predictive power of NAs

It is good practice to split the preprocessing dataset prior to estimating IV. This allows you to adjust the IV estimates using cross-validation, so that variables that have high IV by chance do not get past the filter. See the {Information} package [vignette](https://cran.r-project.org/web/packages/Information/vignettes/Information-vignette.html) for more details.

```{r information value}
set.seed(666)

# split preprocessin data
iv_split <- initial_split(df_pre, 0.5, strata = "purchase")
df_iv_train <- training(iv_split)
df_iv_test <- testing(iv_split)

# calculate IV
iv <- create_infotables(
  data = df_iv_train,
  valid = df_iv_test,
  y = "purchase"
)
```

Notice that the `unique_id` variable was ignored because it has too many levels. This is a handy feature of the {Information} package when dealing with large datasets. It automatically ignores "junk" variables, like customer IDs and zip codes. Anything that's non-numeric with more than 1,000 levels does not get included in the IV estimation process.

The `create_infotables()` function will create a data frame (accessible via `$Summary`) with IV estimates for each predictor, along with a cross-validation penalty and the adjusted IV score.

Once you have the IV estimates, you will need to pick a threshold for excluding variables based on adjusted IV. This is subjective. But in general, the rule of thumb is:

| IV  |  Predictive Power |
|---|---|
| <0.02  |  useless |
| 0.02 to 0.1  |  weak |
| 0.1 to 0.3  | medium  |
| 0.3 to 0.5  |  strong |
|  >0.5 |  suspicious |

**You don't want to be too restrictive at this stage**, especially if you are using a modeling approach that has a built-in feature selection process, as is the case with tree-based algorithms Typically, I would just drop all variables with IV <0.02 because they're probably not going to add anything but noise. However, if most of the variables have relatively low IV scores, I would just take the `top_n()` and hope for the best.

```{r top predictors}
# get top predictors
top_iv <- iv$Summary  %>% 
  filter(AdjIV > 0.02) 

# save predictor names for filtering
top_nm <- as.character(top_iv$Variable)

top_iv
```

As you can see, filtering by adjusted IV reduces the number of predictors in our example dataset to 41. That implies that 37% of the original 65 predictors where probably "useless."

## Feature engineering with {vtreat}

The [vtreat](https://winvector.github.io/vtreat/) package will automatically:

* Replace NAs with mean (numeric) or majority class (non-numeric)
* Create missing-indicator variables
* Dummy code all non-numeric variables with frequency >2% (rare levels grouped together)
* Truncate numeric distributions to mitigate outliers
* Create derived versions of non-numeric variables using
  * Prevalance coding
  * [Impact coding](https://win-vector.com/2012/07/23/modeling-trick-impact-coding-of-categorical-variables-with-many-levels/)

Prevalence coding replaces the levels of a categorical variable with the fraction each level is observed in the dataset. Impact coding uses the marginal effect from a single-variable logistic regression as a replacement for each level in a categorical variable.

These transformations should be reproducible in SQL using case-when statements.

### Create treatment plan

Use the `designTreatmentsC()` function to create a variable treatment plan for classification models. There are a lot of options for this function, so check the documentation.

```{r create treatment plan}
df_vtreat <- df_pre %>% 
  select(all_of(top_nm), purchase) 

vtreat_plan <- designTreatmentsC(
  dframe = df_vtreat,
  varlist = top_nm,
  outcomename = "purchase",
  outcometarget = 1,
  collarProb = .025
)
```

### Prepare training data

After you have the treatment plan object, you can apply it to a new dataset using `prepare()`. This creates a new data frame with treated variables based on the `codeRestriction` argument. Save the treatment plan (`vtreat_plan`) as an .RDS object so you can apply it to the test data prior to generating predictions with the XGBoost model object.

```{r treat training data}
df_train2 <- prepare(
  treatmentplan = vtreat_plan,
  dframe = df_train,
  codeRestriction = c("clean", "lev", "catB", "catP", "isBAD"),
  doCollar = TRUE
)
```

### Derived variables

Notice that the original `d_region` character variable has been transformed into 4 dummy indicators, plus 2 derived variables based on prevalence ("catP") and impact coding ("catB"). The `prepare()` function will do this for every non-numeric variable in the dataset.

```{r check d_region}
df_train2 %>% 
  select(contains("d_region")) %>% 
  head()
```

## Remove redundant variables

Redundant variables are predictors that are highly correlated with one or more other predictors in the dataset.

From a predictive accuracy standpoint, it is not strictly necessary to remove redundant variables prior to model fitting. This is one of the many distinctions between [predictive and explanatory modeling](https://www.stat.berkeley.edu/~aldous/157/Papers/shmueli.pdf).

However, when using tree-based algorithms, **it is necessary to remove redundant predictors in order to get accurate estimates of variable importance**. As noted by Khun and Johnson,

> If there are highly correlated predictors in a training set that are useful for predicting the outcome, then which predictor is chosen is 
> essentially a random selection. It is common to see that a set of highly redundant and useful predictors are all used in the splits across the 
> ensemble of trees. In this scenario, the predictive performance of the ensemble of trees is unaffected by highly correlated, useful features. 
> However, the redundancy of the features dilutes the importance scores.

If you are doing an initial run of XGBoost, taking the top 50 predictors based on variable importance, and then tuning the XGBoost parameters using those predictors, then removing redundant variables prior to model fitting could help.

To find the most redudant features in a dataset, you can use the `findCorrelation()` function from the [{Caret}](https://topepo.github.io/caret/) package.

```{r find correlated}
corr_vars <- findCorrelation(
  cor(
    df_train2,
    method = "spearman"
  ),
  cutoff = 0.9,
  names = TRUE,
  exact = TRUE
)

corr_vars
```

## Final training set

After removing `r length(corr_vars)` redundant variables, the final dataset has 43 predictors that are all numeric and ready for model training.

```{r final dataset}
df_train3 <- df_train2 %>% select(-all_of(corr_vars))

str(df_train3)
```